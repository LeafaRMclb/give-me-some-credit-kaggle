{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from modules.training_models import (\n",
    "    infer, cv_train_model,\n",
    "    lr_cv, nn_cv, rf_cv, lgb_cv, xgb_cv\n",
    ")\n",
    "\n",
    "from modules.training_utils import (\n",
    "    round_float_to, get_round_num,\n",
    "    get_opt_val_seeds, make_sub, timing\n",
    ")\n",
    "\n",
    "from modules.training_optimizers import (\n",
    "    get_optimized_weighted_preds_for\n",
    ")\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='output.logs', format='%(asctime)s: %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "train = os.path.join(DATA_DIR,'transformed-train.csv')\n",
    "test = os.path.join(DATA_DIR, 'transformed-test.csv')\n",
    "\n",
    "train = pd.read_csv(train)\n",
    "test = pd.read_csv(test)\n",
    "for dataset in [train, test]:\n",
    "    dataset.drop(\"Unnamed: 0\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from six.moves import cPickle\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from collections import Counter\n",
    "from bayes_opt import BayesianOptimization\n",
    "from hashlib import md5\n",
    "\n",
    "\n",
    "def vprint(x):\n",
    "    if log_verbose:\n",
    "        logging.info(x)\n",
    "\n",
    "\n",
    "def prepare_test_data(X_test=None, X_train=None, X_val=None, fill='mean'):\n",
    "    # http://fastml.com/how-to-use-pd-dot-get-dummies-with-the-test-set/\n",
    "\n",
    "    if X_val is None:\n",
    "        X_test = test\n",
    "    else:\n",
    "        X_test = X_val\n",
    "\n",
    "    for col in X_train.columns.difference(X_test.columns):\n",
    "        if fill == 'mean':\n",
    "            X_test[col] = X_train[col].mean()\n",
    "        elif fill == None:\n",
    "            X_test[col] = np.nan\n",
    "        else:\n",
    "            raise ValueError('Unknow fill method!')\n",
    "\n",
    "    X_test = X_test[X_train.columns]\n",
    "\n",
    "    if fill == 'mean':\n",
    "        X_test = X_test.fillna(X_train.mean())\n",
    "    elif fill == None:\n",
    "        # Do nothing since missing data is NaN already.\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError('Unknow fill method!')\n",
    "\n",
    "    return X_test\n",
    "\n",
    "\n",
    "def get_train_X_y(indiv_corr_thresh):\n",
    "    '''\n",
    "    hhold_train, indiv_train, indiv_cat_train, and country_code are global variables!\n",
    "    '''\n",
    "\n",
    "    X_train = train.drop('SeriousDlqin2yrs', axis=1).copy()\n",
    "    y_train = train.SeriousDlqin2yrs\n",
    "\n",
    "    return X_train, y_train\n",
    "\n",
    "\n",
    "def build_data_subset(indiv_corr_thresh, opt_val_seed=1029):\n",
    "    if indiv_corr_thresh >= 0:\n",
    "        X_train, y_train = get_train_X_y(indiv_corr_thresh=indiv_corr_thresh)\n",
    "        X_train, X_opt_val, y_train, y_opt_val = train_test_split(\n",
    "           X_train, y_train, test_size=0.05, random_state=opt_val_seed\n",
    "        )\n",
    "    else:\n",
    "        X_train, y_train = get_train_X_y(indiv_corr_thresh=0)\n",
    "        X_train, X_opt_val, y_train, y_opt_val = train_test_split(\n",
    "           X_train, y_train, test_size=0.05, random_state=42\n",
    "        )\n",
    "        X_train, X_feat_sel, y_train, y_feat_sel = train_test_split(\n",
    "            X_train, y_train, test_size=0.05, random_state=opt_val_seed\n",
    "        )\n",
    "\n",
    "        feat_C = 10. ** (round(abs(indiv_corr_thresh), 1) * 10)\n",
    "\n",
    "        br = LogisticRegression(C=feat_C, penalty='l1', random_state=42)\n",
    "        br.fit(X_feat_sel, y_feat_sel)\n",
    "\n",
    "        important_feats = X_feat_sel.columns[br.coef_[0] > 0]\n",
    "        X_train = X_train[important_feats]\n",
    "        \n",
    "    X_train = X_train[X_train.columns[X_train.std() != 0]]\n",
    "    vprint('Using {} features :: opt_val_seed {}...'.format(X_train.shape[1], opt_val_seed))\n",
    "    \n",
    "    return (X_train, y_train), (X_opt_val, y_opt_val)\n",
    "\n",
    "def bayesian_optimize_model(model_type, tunable_params=None, num_iter=20, init_points=0):\n",
    "    global round_num\n",
    "    global is_training\n",
    "\n",
    "    round_num = 0\n",
    "    is_training = True\n",
    "\n",
    "    store_fname = 'BO_res_{}_optimization.dict'.format(model_type)\n",
    "    logging.info('Bayesian optimization results will be stored in {} after training...'.format(store_fname))\n",
    "\n",
    "    if tunable_params is None:\n",
    "        if model_type == 'xgb':\n",
    "            tunable_params = {\n",
    "                'indiv_corr_thresh': (0, 0.3),\n",
    "                'colsample_bytree': (0.2, 1),\n",
    "                'max_depth': (2, 6),\n",
    "                'subsample': (0.2, 1),\n",
    "                'gamma': (0, 2),\n",
    "                'scale_pos_weight': (0, 1),\n",
    "            }\n",
    "        elif model_type == 'lr':\n",
    "            tunable_params = {\n",
    "                'indiv_corr_thresh': (-0.9, 0.3),\n",
    "                'C': (0.001, 0.6),\n",
    "            }\n",
    "        elif model_type == 'lgb':\n",
    "            tunable_params = {\n",
    "                'indiv_corr_thresh': (0, 0.4),\n",
    "                'reg_alpha': (0.0, 1),\n",
    "                'reg_lambda': (0.0, 1),\n",
    "                'num_leaves': (4, 64),\n",
    "                'colsample_bytree' : (0.2, 1),\n",
    "                'subsample' : (0.2, 1),\n",
    "                'min_child_samples': (2, 120),\n",
    "                'scale_pos_weight': (0, 1),\n",
    "            }\n",
    "        elif model_type == 'nn':\n",
    "            tunable_params = {\n",
    "                'indiv_corr_thresh': (-0.9, 0.3),\n",
    "                'l1_num': (0, 100),\n",
    "                'l2_num' : (0, 100),\n",
    "                'l3_num' : (0, 100),\n",
    "                'alpha' : (0.005, 0.1),\n",
    "            }\n",
    "\n",
    "        elif model_type == 'rf':\n",
    "            tunable_params = {\n",
    "                'indiv_corr_thresh': (0, 0.4),\n",
    "                'max_depth': (5, 15),\n",
    "                'min_samples_split' : (2, 20),\n",
    "                'min_samples_leaf' : (2, 20),\n",
    "            }\n",
    "\n",
    "    if model_type == 'xgb':\n",
    "        model_predict = xgb_predict\n",
    "    elif model_type == 'lr':\n",
    "        model_predict = lr_predict\n",
    "    elif model_type == 'lgb':\n",
    "        model_predict = lgb_predict\n",
    "    elif model_type == 'nn':\n",
    "        model_predict = nn_predict\n",
    "    elif model_type == 'rf':\n",
    "        model_predict = rf_predict\n",
    "\n",
    "    modelBO = BayesianOptimization(\n",
    "        model_predict, tunable_params, verbose=bayes_opt_verbose\n",
    "    )\n",
    "\n",
    "    modelBO.maximize(init_points=init_points, n_iter=num_iter, acq=\"poi\", xi=0.1)\n",
    "\n",
    "    with open('bayesian-opts-res/{}'.format(modelBO.res['max']['max_val']) + '-' + store_fname, 'wb') as fl:\n",
    "        cPickle.dump(modelBO.res, fl)\n",
    "\n",
    "    return modelBO\n",
    "\n",
    "\n",
    "def infer_test_val(\n",
    "    model_id, res_name, X_train, y_oof_preds,\n",
    "    X_test, X_val, fill_type, classifiers\n",
    "    ):\n",
    "    X_test = prepare_test_data(\n",
    "        X_test=X_test,\n",
    "        X_train=X_train, fill=fill_type\n",
    "    )\n",
    "\n",
    "    test_payload = infer(model_id, res_name, X_test, fill_type, classifiers)\n",
    "\n",
    "    payload = {\n",
    "        'train': {\n",
    "            '{}_{}'.format(model_id, res_name): y_oof_preds,\n",
    "            'index': X_train.index\n",
    "        },\n",
    "        'test': test_payload\n",
    "    }\n",
    "\n",
    "    if X_val is not None:\n",
    "        X_val = prepare_test_data(\n",
    "            X_train=X_train, X_val=X_val, fill=fill_type\n",
    "        )\n",
    "        val_payload = infer(model_id, res_name, X_val, fill_type, classifiers)\n",
    "\n",
    "        payload['val'] = val_payload\n",
    "\n",
    "    return payload\n",
    "\n",
    "\n",
    "def lr_predict(\n",
    "    indiv_corr_thresh,\n",
    "    C,\n",
    "    class_weight=None,\n",
    "    X_test=None,\n",
    "    X_val=None,\n",
    "    res_name=None,\n",
    "    opt_val_seed=None,\n",
    "    model_id='lr',\n",
    "    cv_func=lr_cv,\n",
    "    fill_type='mean'\n",
    "):\n",
    "    indiv_corr_thresh = round(indiv_corr_thresh, 2)\n",
    "\n",
    "    if opt_val_seed is None:\n",
    "        global round_num\n",
    "        global opt_val_seeds\n",
    "        opt_val_seed = opt_val_seeds[round_num]\n",
    "\n",
    "    (X_train, y_train), (X_opt_val, y_opt_val) = build_data_subset(\n",
    "        indiv_corr_thresh=indiv_corr_thresh, opt_val_seed=opt_val_seed\n",
    "    )\n",
    "\n",
    "    X_opt_val = prepare_test_data(\n",
    "        X_train=X_train, X_val=X_opt_val, fill=fill_type\n",
    "    )\n",
    "\n",
    "    params = {}\n",
    "    params['C'] = max(round(C, 2), 0.001)\n",
    "    if class_weight is not None:\n",
    "        params['class_weight'] = {\n",
    "            0: 1.0,\n",
    "            1: round(class_weight, 2),\n",
    "        } if class_weight < 0.5 else None\n",
    "\n",
    "    params['penalty'] = 'l1'\n",
    "\n",
    "    y_oof_preds, classifiers, opt_val_auc = cv_train_model(\n",
    "        X_train, y_train, X_opt_val, y_opt_val, params,\n",
    "        model_id=model_id, res_name=res_name, cv_func=cv_func,\n",
    "        fill_type=fill_type, cv_split=10\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        payload = infer_test_val(\n",
    "            model_id=model_id, res_name=res_name,\n",
    "            X_train=X_train, y_oof_preds=y_oof_preds,\n",
    "            X_test=X_test,\n",
    "            X_val=X_val,\n",
    "            fill_type=fill_type,\n",
    "            classifiers=classifiers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return payload\n",
    "    else:\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return opt_val_auc\n",
    "\n",
    "\n",
    "def nn_predict(\n",
    "    indiv_corr_thresh,\n",
    "    l1_num,\n",
    "    l2_num,\n",
    "    alpha,\n",
    "    l3_num=1,\n",
    "    X_test=None,\n",
    "    X_val=None,\n",
    "    res_name=None,\n",
    "    opt_val_seed=None,\n",
    "    model_id='nn',\n",
    "    cv_func=nn_cv,\n",
    "    fill_type='mean'\n",
    "):\n",
    "    indiv_corr_thresh = round(indiv_corr_thresh, 2)\n",
    "\n",
    "    if opt_val_seed is None:\n",
    "        global round_num\n",
    "        global opt_val_seeds\n",
    "        opt_val_seed = opt_val_seeds[round_num]\n",
    "\n",
    "    (X_train, y_train), (X_opt_val, y_opt_val) = build_data_subset(\n",
    "        indiv_corr_thresh=indiv_corr_thresh, opt_val_seed=opt_val_seed\n",
    "    )\n",
    "\n",
    "    X_opt_val = prepare_test_data(\n",
    "        X_train=X_train, X_val=X_opt_val, fill=fill_type\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        'hidden_layer_sizes': (50, 20, 1),\n",
    "        'alpha': 0.01,\n",
    "        'max_iter': 1000,\n",
    "        'early_stopping': True,\n",
    "        'random_state': 1029\n",
    "    }\n",
    "\n",
    "    params['alpha'] = round(alpha, 2)\n",
    "    params['hidden_layer_sizes'] = (get_round_num(l1_num, 10), get_round_num(l2_num, 10), get_round_num(l3_num, 5))\n",
    "\n",
    "    y_oof_preds, classifiers, opt_val_auc = cv_train_model(\n",
    "        X_train, y_train, X_opt_val, y_opt_val, params,\n",
    "        model_id=model_id, res_name=res_name, cv_func=cv_func,\n",
    "        fill_type=fill_type, cv_split=10\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        payload = infer_test_val(\n",
    "            model_id=model_id, res_name=res_name,\n",
    "            X_train=X_train, y_oof_preds=y_oof_preds,\n",
    "            X_test=X_test,\n",
    "            X_val=X_val,\n",
    "            fill_type=fill_type,\n",
    "            classifiers=classifiers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return payload\n",
    "    else:\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return opt_val_auc\n",
    "\n",
    "\n",
    "def rf_predict(\n",
    "    indiv_corr_thresh,\n",
    "    max_depth,\n",
    "    min_samples_split,\n",
    "    min_samples_leaf,\n",
    "    X_test=None,\n",
    "    X_val=None,\n",
    "    res_name=None,\n",
    "    opt_val_seed=None,\n",
    "    model_id='rf',\n",
    "    cv_func=rf_cv,\n",
    "    fill_type='mean'\n",
    "):\n",
    "    indiv_corr_thresh = round(indiv_corr_thresh, 2)\n",
    "\n",
    "    if opt_val_seed is None:\n",
    "        global round_num\n",
    "        global opt_val_seeds\n",
    "        opt_val_seed = opt_val_seeds[round_num]\n",
    "\n",
    "    (X_train, y_train), (X_opt_val, y_opt_val) = build_data_subset(\n",
    "        indiv_corr_thresh=indiv_corr_thresh, opt_val_seed=opt_val_seed\n",
    "    )\n",
    "\n",
    "    X_opt_val = prepare_test_data(\n",
    "        X_train=X_train, X_val=X_opt_val, fill=fill_type\n",
    "    )\n",
    "\n",
    "    params = dict(\n",
    "        max_depth=20, min_samples_split=20, min_samples_leaf=10, n_jobs=7, random_state=1029\n",
    "    )\n",
    "\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['min_samples_split'] = get_round_num(min_samples_split, 2)\n",
    "    params['min_samples_leaf'] = get_round_num(min_samples_leaf, 2)\n",
    "\n",
    "    y_oof_preds, classifiers, opt_val_auc = cv_train_model(\n",
    "        X_train, y_train, X_opt_val, y_opt_val, params,\n",
    "        model_id=model_id, res_name=res_name, cv_func=cv_func,\n",
    "        fill_type=fill_type, cv_split=10\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        payload = infer_test_val(\n",
    "            model_id=model_id, res_name=res_name,\n",
    "            X_train=X_train, y_oof_preds=y_oof_preds,\n",
    "            X_test=X_test,\n",
    "            X_val=X_val,\n",
    "            fill_type=fill_type,\n",
    "            classifiers=classifiers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return payload\n",
    "    else:\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return opt_val_auc\n",
    "\n",
    "\n",
    "def lgb_predict(\n",
    "    indiv_corr_thresh,\n",
    "    reg_lambda,\n",
    "    reg_alpha,\n",
    "    num_leaves,\n",
    "    colsample_bytree,\n",
    "    subsample,\n",
    "    min_child_samples,\n",
    "    scale_pos_weight,\n",
    "    subsample_for_bin=None,\n",
    "    X_test=None,\n",
    "    X_val=None,\n",
    "    res_name=None,\n",
    "    opt_val_seed=None,\n",
    "    model_id='lgb',\n",
    "    cv_func=lgb_cv,\n",
    "    fill_type='mean'\n",
    "):\n",
    "    indiv_corr_thresh = round(indiv_corr_thresh, 2)\n",
    "\n",
    "    if opt_val_seed is None:\n",
    "        global round_num\n",
    "        global opt_val_seeds\n",
    "        opt_val_seed = opt_val_seeds[round_num]\n",
    "\n",
    "    (X_train, y_train), (X_opt_val, y_opt_val) = build_data_subset(\n",
    "        indiv_corr_thresh=indiv_corr_thresh, opt_val_seed=opt_val_seed\n",
    "    )\n",
    "\n",
    "    X_opt_val = prepare_test_data(\n",
    "        X_train=X_train, X_val=X_opt_val, fill=fill_type\n",
    "    )\n",
    "\n",
    "    params = {\n",
    "        'n_estimators': 1000,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'max_depth' : -1,\n",
    "        'objective': 'binary',\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample_freq': 1,\n",
    "        'scale_pos_weight': 1,\n",
    "        'metric' : 'auc'\n",
    "    }\n",
    "\n",
    "    params['colsample_bytree'] = max(min(round(colsample_bytree, 2), 1), 0)\n",
    "    params['subsample'] = max(min(round(subsample, 2), 1), 0)\n",
    "    params['num_leaves'] = int(num_leaves)\n",
    "    params['min_child_samples'] = int(min_child_samples)\n",
    "    params['subsample_for_bin'] = int(subsample_for_bin) if subsample_for_bin is not None else 200000\n",
    "    params['reg_lambda'] = int(reg_lambda)\n",
    "    params['reg_alpha'] = int(reg_alpha)\n",
    "    params['learning_rate'] = 0.1\n",
    "    params['n_estimators'] = 10000\n",
    "    params['objective'] = 'binary'\n",
    "    params['random_state'] = 1029\n",
    "    params['n_jobs'] = -1\n",
    "    params['silent'] = True\n",
    "\n",
    "    scale_pos_weight = 1 if scale_pos_weight > 0.5 else 1.0 * sum(y_train < 0.5) / sum(y_train > 0.5)\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "    y_oof_preds, classifiers, opt_val_auc = cv_train_model(\n",
    "        X_train, y_train, X_opt_val, y_opt_val, params,\n",
    "        model_id=model_id, res_name=res_name, cv_func=cv_func,\n",
    "        fill_type=fill_type, cv_split=10\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        payload = infer_test_val(\n",
    "            model_id=model_id, res_name=res_name,\n",
    "            X_train=X_train, y_oof_preds=y_oof_preds,\n",
    "            X_test=X_test,\n",
    "            X_val=X_val,\n",
    "            fill_type=fill_type,\n",
    "            classifiers=classifiers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return payload\n",
    "    else:\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return opt_val_auc\n",
    "\n",
    "\n",
    "def xgb_predict(\n",
    "    indiv_corr_thresh,\n",
    "    colsample_bytree,\n",
    "    max_depth,\n",
    "    subsample,\n",
    "    gamma,\n",
    "    scale_pos_weight,\n",
    "    X_test=None,\n",
    "    X_val=None,\n",
    "    res_name=None,\n",
    "    opt_val_seed=None,\n",
    "    model_id='xgb',\n",
    "    cv_func=xgb_cv,\n",
    "    fill_type=None\n",
    "):\n",
    "    '''\n",
    "    DMatrix params\n",
    "    - weight\n",
    "    - missing (-999)\n",
    "\n",
    "    xgboost params\n",
    "    max_depth\n",
    "    min_child_weight\n",
    "    gamma -> minimum loss reduction required to make a further partition on a leaf node of the tree. The larger, the more conservative the algorithm will be.\n",
    "    subsample -> proportion of data\n",
    "    colsample_bytree -> proportion of columns used in each tree\n",
    "    max_delta_step -> for imbalanced use finite value, e.g., 1\n",
    "\n",
    "    Decrease eta and increase nrounds if overfitting is observed.\n",
    "    '''\n",
    "    indiv_corr_thresh = round(indiv_corr_thresh, 2)\n",
    "\n",
    "    if opt_val_seed is None:\n",
    "        global round_num\n",
    "        global opt_val_seeds\n",
    "        opt_val_seed = opt_val_seeds[round_num]\n",
    "\n",
    "    (X_train, y_train), (X_opt_val, y_opt_val) = build_data_subset(\n",
    "        indiv_corr_thresh=indiv_corr_thresh, opt_val_seed=opt_val_seed\n",
    "    )\n",
    "\n",
    "    X_opt_val = prepare_test_data(\n",
    "        X_train=X_train, X_val=X_opt_val, fill=fill_type\n",
    "    )\n",
    "    \n",
    "    params = {}\n",
    "    params['colsample_bytree'] = max(min(round(colsample_bytree, 2), 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(round(subsample, 2), 1), 0)\n",
    "    params['gamma'] = max(round(gamma, 2), 0)\n",
    "    params['learning_rate'] = 0.2\n",
    "    params['n_estimators'] = 10000\n",
    "    params['objective'] = 'binary:logistic'\n",
    "    params['random_state'] = 1029\n",
    "    params['n_jobs'] = -1\n",
    "    params['silent'] = True\n",
    "\n",
    "    scale_pos_weight = 1 if scale_pos_weight > 0.5 else 1.0 * sum(y_train < 0.5) / sum(y_train > 0.5)\n",
    "    params['scale_pos_weight'] = scale_pos_weight\n",
    "\n",
    "    y_oof_preds, classifiers, opt_val_auc = cv_train_model(\n",
    "        X_train, y_train, X_opt_val, y_opt_val, params,\n",
    "        model_id=model_id, res_name=res_name, cv_func=cv_func,\n",
    "        fill_type=fill_type, cv_split=10\n",
    "    )\n",
    "\n",
    "    if X_test is not None:\n",
    "        payload = infer_test_val(\n",
    "            model_id=model_id, res_name=res_name,\n",
    "            X_train=X_train, y_oof_preds=y_oof_preds,\n",
    "            X_test=X_test,\n",
    "            X_val=X_val,\n",
    "            fill_type=fill_type,\n",
    "            classifiers=classifiers\n",
    "        )\n",
    "\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return payload\n",
    "    else:\n",
    "        if is_training:\n",
    "            round_num += 1\n",
    "        return opt_val_auc\n",
    "\n",
    "\n",
    "def predict_from_opt(model_type, num_params=10, use_latest=False, default_dir='bayesian-opts-res/'):\n",
    "    global is_training\n",
    "    is_training = False\n",
    "\n",
    "    candidate_fnames = sorted(glob.glob(os.path.join(default_dir, '*-*_res_{}_*'.format(model_type))))\n",
    "\n",
    "    if use_latest:\n",
    "        fname = max(candidate_fnames)\n",
    "    else:\n",
    "        fname = candidate_fnames[0]\n",
    "\n",
    "    vprint(fname)\n",
    "    fname= fname.encode()\n",
    "    with open(fname, 'rb') as fl:\n",
    "        res = cPickle.load(fl)\n",
    "\n",
    "    if model_type == 'xgb':\n",
    "        model_predict = xgb_predict\n",
    "    elif model_type == 'lr':\n",
    "        model_predict = lr_predict\n",
    "    elif model_type == 'lgb':\n",
    "        model_predict = lgb_predict\n",
    "    elif model_type == 'nn':\n",
    "        model_predict = nn_predict\n",
    "    elif model_type == 'rf':\n",
    "        model_predict = rf_predict\n",
    "\n",
    "    res_df = pd.DataFrame(res['all'])\n",
    "    top_res = res_df['values'].argsort().iloc[-num_params:]\n",
    "    num_round_index = top_res.values[::-1]\n",
    "    params = res_df.loc[top_res.values[::-1]]['params']\n",
    "\n",
    "    model_test_preds = pd.DataFrame()\n",
    "    model_train_preds = pd.DataFrame()\n",
    "\n",
    "    for res_name, (nm_rnd, param) in enumerate(zip(num_round_index, params)):\n",
    "        opt_val_seed = opt_val_seeds[nm_rnd]\n",
    "        r = model_predict(X_test=True, opt_val_seed=opt_val_seed, res_name=res_name, **param)\n",
    "\n",
    "        train_preds = pd.DataFrame(index=r['train']['index'])\n",
    "        train_preds['{}_{}'.format(model_type, res_name)] = r['train']['{}_{}'.format(model_type, res_name)]\n",
    "\n",
    "        test_preds = r['test']['{}_{}'.format(model_type, res_name)]\n",
    "\n",
    "        if model_test_preds.empty:\n",
    "            model_test_preds = test_preds\n",
    "        else:\n",
    "            model_test_preds = pd.concat([model_test_preds, test_preds], axis=1)\n",
    "\n",
    "        if model_train_preds.empty:\n",
    "            model_train_preds = train_preds\n",
    "        else:\n",
    "            model_train_preds = pd.concat([model_train_preds, train_preds], axis=1)\n",
    "\n",
    "    return model_test_preds, model_train_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data directories\n",
    "DATA_DIR = os.path.join('..', 'data')\n",
    "SUBMISSION_DIR = os.path.join('..', 'submission')\n",
    "BAYESIAN_OPTS_DIR = 'bayesian-opts-res'\n",
    "BAYESIAN_OPTS_TEST_DIR = os.path.join(BAYESIAN_OPTS_DIR, 'bayesian-opt-test-preds')\n",
    "\n",
    "# Setup directories\n",
    "if not os.path.isdir(BAYESIAN_OPTS_DIR):\n",
    "    logging.info('Creating {} directory...'.format(BAYESIAN_OPTS_DIR))\n",
    "    os.mkdir(BAYESIAN_OPTS_DIR)\n",
    "\n",
    "if not os.path.isdir(BAYESIAN_OPTS_TEST_DIR):\n",
    "    logging.info('Creating {} directory...'.format(BAYESIAN_OPTS_TEST_DIR))\n",
    "    os.mkdir(BAYESIAN_OPTS_TEST_DIR)\n",
    "\n",
    "if not os.path.isdir(SUBMISSION_DIR):\n",
    "    logging.info('Creating {} directory...'.format(SUBMISSION_DIR))\n",
    "    os.mkdir(SUBMISSION_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bayes_opt_verbose = True  # Flag for bayes-opt logging\n",
    "log_verbose = True  # Flag for miscellaneous log logging\n",
    "init_points = 2  # Number of random initialization rounds (30 used in winning submission)\n",
    "opt_iter = 2  # Number of bayesian optimization rounds (70 used in winning submission)\n",
    "\n",
    "num_gen_params =  2  # Number of top meta predictions to use for blending (20 used in winning submission)\n",
    "\n",
    "round_num = None  # Global variable to keep track of the bayesian optimization round\n",
    "is_training = None  # Global variable to indicate training or testing periods\n",
    "\n",
    "#session = datetime.now()\n",
    "opt_val_seeds = get_opt_val_seeds(init_points * opt_iter)  # Generation of seeds for each bayesian optimization round\n",
    "\n",
    "model_set = ['lr','nn','xgb', 'rf', 'lgb']\n",
    "\n",
    "opt_res = dict()\n",
    "\n",
    "preds_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute training and meta prediction inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timing('Performing training and meta predictions'):\n",
    "    x_model_train_preds = pd.DataFrame()\n",
    "    x_model_test_preds = pd.DataFrame()\n",
    "    with timing('Start bayesian optimization and generation of optimal sub-models'):\n",
    "        for ix, model_type in enumerate(model_set):\n",
    "            with timing('{}. Training bayesian opts for {} model.'.format(ix + 1, model_type)):\n",
    "                modelBO = bayesian_optimize_model(model_type=model_type, tunable_params=None, num_iter=opt_iter, init_points=init_points)\n",
    "            opt_res[model_type] = modelBO.res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with timing('Performing training and meta predictions'):\n",
    "    x_model_train_preds = pd.DataFrame()\n",
    "    x_model_test_preds = pd.DataFrame()\n",
    "    with timing('Start bayesian optimization and generation of optimal sub-models'):\n",
    "        for ix, model_type in enumerate(model_set):\n",
    "            with timing('{}. Training bayesian opts for {} model.'.format(ix + 1, model_type)):\n",
    "                modelBO = bayesian_optimize_model(model_type=model_type, tunable_params=None, num_iter=opt_iter, init_points=init_points)\n",
    "            opt_res[model_type] = modelBO.res\n",
    "    fname = 'bayesian-opts-res/bayesian_opt_res_results.pkl'\n",
    "    vprint(fname)\n",
    "\n",
    "    with open(fname, 'wb') as fl:\n",
    "        cPickle.dump(opt_res, fl)\n",
    "        \n",
    "    with timing('Generate inference using the top {} models based on bayesian OOF score'.format(num_gen_params)):\n",
    "        for ix, model_type in enumerate(model_set):\n",
    "            with timing('{}. Generating meta predictions for {} model.'.format(ix + 1, model_type)):\n",
    "                model_test_preds, model_train_preds = predict_from_opt(\n",
    "                    model_type, num_params=num_gen_params,\n",
    "                    use_latest=True, default_dir=BAYESIAN_OPTS_DIR\n",
    "                )\n",
    "\n",
    "\n",
    "                model_test_preds.to_csv(\n",
    "                    os.path.join(BAYESIAN_OPTS_TEST_DIR, 'model_test_preds.csv'),\n",
    "                    'model_test_preds_{}'.format(model_type)\n",
    "                )\n",
    "\n",
    "                model_train_preds.to_csv(\n",
    "                    os.path.join(BAYESIAN_OPTS_TEST_DIR, 'model_train_preds.csv'),\n",
    "                    'model_train_preds{}'.format(model_type)\n",
    "                )\n",
    "\n",
    "                # Collect meta predictions\n",
    "                x_model_train_preds = pd.concat([x_model_train_preds, model_train_preds], axis=1)\n",
    "                x_model_test_preds = pd.concat([x_model_test_preds, model_test_preds], axis=1)\n",
    "    \n",
    "    preds_dict['train'] = x_model_train_preds\n",
    "    preds_dict['test'] = x_model_test_preds\n",
    "    preds_dict['y_train'] = train.loc[x_model_train_preds.index].SeriousDlqin2yrs.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform OOF optimized blending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "median_weight_optimized_preds_sub = []\n",
    "mean_weight_optimized_preds_sub = []\n",
    "\n",
    "\n",
    "with timing('Optimizing meta prediction weights'):\n",
    "    x_test_median_preds, x_test_mean_preds, x_optimized_weights_preds, x_coeffs = get_optimized_weighted_preds_for(preds_dict)\n",
    "\n",
    "median_weight_optimized_preds_sub.append(make_sub(x_test_median_preds, x_test_median_preds))\n",
    "mean_weight_optimized_preds_sub.append(make_sub(x_test_mean_preds, x_test_mean_preds))\n",
    "\n",
    "median_weight_optimized_preds_sub = pd.concat(median_weight_optimized_preds_sub)\n",
    "mean_weight_optimized_preds_sub = pd.concat(mean_weight_optimized_preds_sub)\n",
    "\n",
    "\n",
    "date_now = datetime.now()\n",
    "median_sub_fname = os.path.join(SUBMISSION_DIR, 'submission-test-median-agg-models.csv')\n",
    "mean_sub_fname = os.path.join(SUBMISSION_DIR, 'submission-test-mean-agg-models.csv')\n",
    "\n",
    "logging.info(median_sub_fname)\n",
    "logging.info(mean_sub_fname)\n",
    "\n",
    "median_weight_optimized_preds_sub.to_csv(median_sub_fname, index=False)\n",
    "mean_weight_optimized_preds_sub.to_csv(mean_sub_fname, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
